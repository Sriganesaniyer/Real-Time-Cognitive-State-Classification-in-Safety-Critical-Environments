**Project Overview**

Safety-critical environments such as aerospace operations, military command systems, and mission-critical industrial settings demand continuous human vigilance and precise decision-making. Even brief lapses in cognitive performance—arising from fatigue, stress, distraction, or overload—can significantly compromise safety and operational integrity. Traditional approaches to monitoring cognitive states often depend on EEG or physiological sensors, which, while accurate, are intrusive, expensive, and impractical for real-world continuous use. To address these limitations, this project proposes a real-time, vision-based cognitive state classification framework that leverages facial landmark dynamics as a non-intrusive indicator of human mental states. By capturing temporal variations in eye and facial movements through standard webcams, the system extracts structured landmark sequences that encode subtle behavioral cues linked to cognitive conditions. These sequences are then modeled using a Long Short-Term Memory (LSTM) neural network, which learns to classify users into one of several cognitive states—alert, fatigued, stressed, distracted, or neutral—in real time. The proposed system aims to provide a cost-effective, scalable, and non-intrusive alternative to biosensor-based monitoring, enabling continuous assessment of operator alertness and cognitive well-being in safety-critical environments.
